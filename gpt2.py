# -*- coding: utf-8 -*-
"""GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-ptLzeW37hd9GjphRARm2uLQuE7Ti96
"""

#Install dependencies
# transformers --> to load and fine-tune pre-trained transformer models
# datasets --> to get a dataset to do specific tasks
# accelerate --> To make training faster and more efficient across different hardware setups
!pip install transformers datasets accelerate

from datasets import load_dataset #import the dataset

dataset = load_dataset("ag_news" , split = "train[:5%]") # Loads the ag_news data and we want only 5% of data

dataset = dataset.map(lambda x : {"text" : x["text"]} ,  remove_columns=dataset.column_names) # This code takes only text column from ag_news dataset

# AutoTokenizer --> It will install the Tokenizer function which are going to help model
# AutoModelForCausalLM --> predicts the next word in a sequence, based on previous words
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "GPT2" # Load the GPT model

tokenizer = AutoTokenizer.from_pretrained(model_name) # Downloads and loads the tokenizer for the specified model
model = AutoModelForCausalLM.from_pretrained(model_name) # Loads the pretrained model weights and architecture for causal language modeling.

tokenizer.pad_token = tokenizer.eos_token # Add this layer because this layer doesn't contain the padding layer
model.resize_token_embeddings(len(tokenizer))

def tokenize_function(example):
  return tokenizer(example["text"], truncation = True, padding ="max_length" , max_length = 128)

tokenized_dataset = dataset.map(tokenize_function, batched = True) # This whole code takes a batch of examples from dataset for tokenzation.

from transformers import TrainingArguments, Trainer

# Set Training Arguments
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned-yelp",
    per_device_train_batch_size=2,
    num_train_epochs=1,
    logging_steps=10,
    save_steps=50,
    save_total_limit=1,
    #evaluation_strategy="no",
    fp16=False,  
    push_to_hub=False,
)

from transformers import Trainer,DataCollatorForLanguageModeling

data_collector = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collector,
)

trainer.train()

trainer.train()





